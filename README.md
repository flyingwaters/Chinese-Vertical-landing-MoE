<div align="center">
    <h1>
        Chinese-Vertical-landing-MoE
    </h1>
</div>

<div align=center>
    <img width="400" height="300" src="./pictures/logo.png"/>
</div>

## ğŸš€ ä»‹ç»

æœ¬é¡¹ç›®åŸºäº[Chinese-mixtral-8*7b](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B)è€Œæ¥, é’ˆå¯¹å‚ç›´è¡Œä¸šæ•°æ®åšäº†tokens å¢åŠ ç®—æ³•å’Œåˆå¹¶tokenizerçš„åŠŸèƒ½ï¼Œæ”¯æŒåœ¨å‚ç›´è¡Œä¸šçš„æ–‡æœ¬è¯­æ–™ä¸Šåšè¯è¡¨æ‰©å……åçš„å¢é‡é¢„è®­ç»ƒï¼Œä»¥åŠæŒ‡ä»¤å¾®è°ƒï¼ˆinstruction tuningï¼‰çš„è®­ç»ƒã€‚Chinese-mixtral-8*7b æ˜¯å“ˆå·¥å¤§æ£€ç´¢ä¿¡æ¯ä¸­å¿ƒåŸºäºMistralå‘å¸ƒçš„æ¨¡å‹[Mixtral-8x7B](https://mistral.ai/news/mixtral-of-experts/)è¿›è¡Œäº†ä¸­æ–‡æ‰©è¯è¡¨å¢é‡é¢„è®­ç»ƒï¼Œå¸Œæœ›è¿›ä¸€æ­¥ä¿ƒè¿›ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç¤¾åŒºå¯¹MoEæ¨¡å‹çš„ç ”ç©¶ã€‚æˆ‘ä»¬æ‰©å……åçš„è¯è¡¨æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ä¸­æ–‡çš„ç¼–è§£ç æ•ˆç‡ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼€æºè¯­æ–™å¯¹æ‰©è¯è¡¨æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å…·å¤‡äº†å¼ºå¤§çš„ä¸­æ–‡ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚

ç›®å‰å¼€æºå†…å®¹ï¼š

- Chinese-Mixtral-8*7Bçš„ä»£ç 
- åˆå¹¶è¯è¡¨å’Œalpè¯è¡¨åˆå¹¶ç®—æ³•
- æŒ‡ä»¤å¾®è°ƒçš„ä»£ç 
- è¯è¡¨åˆå¹¶å’Œå»é‡çš„ç®—æ³•

æœªæ¥å¼€æºå†…å®¹
- åŸºäºgroq æ¡†æ¶çš„ä¸€é”®é«˜æ•ˆéƒ¨ç½²
- å‚ç›´é¢†åŸŸçš„æ•°æ®çš„çˆ¬å–ï¼Œè¿‡æ»¤å’Œæ¸…æ´—æ¡†æ¶
- é›†æˆé‡åŒ–å‹ç¼©æ¡†æ¶


> è¯·æ³¨æ„ï¼ŒChinese-Mixtral-8x7Bä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨ç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†ç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚

## ğŸ“¥ æ¨¡å‹ä¸‹è½½

Chinese-Mixtral-8*7b é¡¹ç›®ä½¿ç”¨QLoRAè¿›è¡Œè®­ç»ƒï¼ŒLoRAæƒé‡ä¸åˆå¹¶æƒé‡åçš„æ¨¡å‹åˆ†åˆ«å¼€æºï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©ä¸‹è½½ï¼š

|             æ¨¡å‹åç§°             | æ¨¡å‹å¤§å°  |                                     ä¸‹è½½åœ°å€                                      |                                                         å¤‡æ³¨                                                          |
|:----------------------------:|:-----:|:-----------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------:|
|     Chinese-Mixtral-8x7B     | 88GB  |     [HuggingFace](https://huggingface.co/HIT-SCIR/Chinese-Mixtral-8x7B)<br>[ModelScope](https://modelscope.cn/models/HIT-SCIR/Chinese-Mixtral-8x7B/summary)     |                                                  ä¸­æ–‡æ‰©è¯è¡¨å®Œæ•´æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨                                                   |
| Chinese-Mixtral-8x7B-adapter | 2.7GB | [HuggingFace](https://huggingface.co/HIT-SCIR/Chinese-Mixtral-8x7B-adapter) | LoRAæƒé‡ï¼Œéœ€è¦ä¸åŸç‰ˆMixtral-8x7Bè¿›è¡Œåˆå¹¶æ‰å¯ä»¥ä½¿ç”¨ï¼Œåˆå¹¶è„šæœ¬è¯·å‚è€ƒ[è¿™é‡Œ](https://gist.github.com/ChrisHayduk/1a53463331f52dca205e55982baf9930) |

## ğŸ’» æ¨¡å‹æ¨ç†

Chinese-Mixtral-8x7Bæ”¯æŒå®Œæ•´çš„Mixtral-8x7Bæ¨¡å‹ç”Ÿæ€ï¼ŒåŒ…æ‹¬ä½¿ç”¨`vLLM`ã€`Flash Attention 2`è¿›è¡ŒåŠ é€Ÿï¼Œä½¿ç”¨`bitsandbytes`è¿›è¡Œæ¨¡å‹é‡åŒ–ç­‰ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨Chinese-Mixtral-8x7Bè¿›è¡Œæ¨ç†çš„ä»£ç ç¤ºä¾‹ã€‚

å¦‚inference.py ä¸­æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ¨ç†æ–¹æ³•

ä½¿ç”¨Flash Attention 2ï¼š
```python
import os
os.environ["CUDA_VISIBLE_DEVICES"]="4,5,6,7"
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM
# Load peft config for pre-trained checkpoint etc.
peft_model_id = "/xxx/models/HIT-SCIR/Chinese-Mixtral-8x7B"
config = PeftConfig.from_pretrained(peft_model_id)

model_path = "/xxx/models/HIT-SCIR/Chinese-Mixtral-8x7B"

# load base LLM model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_path, attn_implementation="flash_attention_2", torch_dtype=torch.bfloat16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

# Load the Lora model
model = PeftModel.from_pretrained(model, peft_model_id, device_map="auto")
model.eval()
```

ä½¿ç”¨4bité‡åŒ–ï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "HIT-SCIR/Chinese-Mixtral-8x7B"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map="auto")

text = "æˆ‘çš„åå­—æ˜¯"
inputs = tokenizer(text, return_tensors="pt").to(0)

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

è¯·æ³¨æ„ï¼ŒChinese-Mixtral-8x7Bä¸ºåŸºåº§æ¨¡å‹ï¼Œæ²¡æœ‰ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå› æ­¤æŒ‡ä»¤éµå¾ªèƒ½åŠ›æœ‰é™ã€‚æ‚¨å¯ä»¥å‚è€ƒ[å¾®è°ƒ](#å¾®è°ƒ)ä¸€èŠ‚å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

### æ¨¡å‹ç»¼åˆèƒ½åŠ›

